- programs must be brought (from disk) into memory and placed within a process for it to be run 
- main memory and registers are the only storage the CPU can access directly
- memory unit only sees a stream of addresses and read requests or address and data + write requests
- cache sits between main memory and cpu registers 

memory itself is not a complicated thing. The complexity comes from the rate of interaction with the cpu. 
#  Memory Hierarchy 
![[Screen Shot 2024-03-11 at 10.48.16 AM.png]]

### CPU Memory
- working memory for the instruction
- general purpose register for storing operands and pointers
- special purpose register for holding the status of a program such as a program counter 

### Cache Memory
- positioned logically between the cpu registers and main memory 
- special memory that compensates the speed mismatch between processor and main memory access time 
- the cache is usually transparent to programmers 

### Main Memory 
- large and fairly fast external memory used to store programs
- storage locations are addressed directlly by the load and store instructions of the cpu
- two modes
	- static ram - a fast access time to implement cache memory 
		- static storage with power on
	- dynamic ram - high density -> more capacity
		- dynamic refresh to overcome current leakage 

### Secondary Memory 
- very large in capacity
- slowest memory type 

# Address Space 
the logical address space which is the set of all logical addresses generated for a program
- logical address - an individual address within the logical address space **this is generated by the cpu during program execution**
- physical address space
	- the complete set of physical memory addresses that a program can generate during its execution 
	- physical address - the address seen by the memory unit 

### Big Picture 
![[Screen Shot 2024-03-11 at 10.59.24 AM.png]]

### Base Register
register to specify the smallest physical memory address for a given process 
- lloaded only by the operation system

### Limit Address
register to specify the size of the range of addresses (defines the size of addresses) 
- loaded only by the operation system

### Logical address space
the set of all logical addresses generated by a programs perspective. Used to support concurrency and avoid collisions 

### Loading a Program into Memory
- programs on disk ready to be brought into memory to execute from an input queue 
- without support, it must be loaded into address 0000
- we can solve this by translating the provided logical address space to actual physical memory address spaces to execute the program

### Address Binding 
the process of mapping from one address space to another address space by the Memory Management Unit (MMU) in the CPU
- mapping logical addresses (generated by the cpu) to physical addresses
- address binding at 3 different stages
	- compile time (by the compiler)
	- load time 
	- execution time

### Compile Time Binding
the logical address of the program is fixed into a physical memory address at compile time
- simple, no runtime overhead fo address translation
- lack of flexibility, no support for dynamic memory allocation
- generated physical addresses are for determining specific memory addresses to variables, functions and instructions in programs -> they are used for generating absolute code which contains the physical address 

### Load -Time Address Binding
the mapping to physical addresses during the loading of a program 
- logical addresses are typically translated to physical addresses by adding an offset to a base address 
	- generating relocatable code with relative addresss (offset) to base address
	- allowing multiple processes to be loaded into memory at the same time without conflicts 
- support for dynamic memory allocation 

### Execution-Time Address Binding
the process of mapping logical addresses to physical addresses during the actual execution of a program
- logical addresses are typically translated to physical addresses bby adding an offset to a base address
- common in modern operating systems 
- it first loads the program into **any** **available** **location** and then it determines the physical address at runtime 

### Memory Management Unit (MMU)
a hardware device that maps the logical address to physical addresses -> access binding 
- works by adding a value in the base register (called relocation register) with the address thats passed in and that is the location of memory that will be accessed 

### Dynamic Loading 
the limitation of loading into memory with the naive implementation 
- the size of a processes has thus been limited to the size of physical memory 
- better memory utilization 
- **routine is not loaded until it is called**
- the relocatable linking loader is called to load the desired routine 
	- responsible for loading the desired routine into memory and updating the programs address tables accordingly
	- part of the MMU is the OS's Kernel 

### Advantages 
- routine with large size but infrequent use may not be loaded always 
- the responsibility is on the users to realize with some library routines
	- the developer must specify which libraries or modules are needed 
	- the os handles the core functionality of loading and unloading libraries at runtime 
when the developers specify the loading needs of the program, the program will not need to use all of its allocated space


### Loading 
the process of copying a loadable image into memory, connecting it with any other programs already loaded and updating address as needed 

### Linking 
combining a set of programs to create a loadable image

![[Screen Shot 2024-03-13 at 10.40.02 AM.png]]

### Dynamically Linked Libraries (DLLs)
system of libraries that are linked to user programs when the programs are run 
- aka shared libraries
- dlls can be considered a collection of loadable modules
- linking is postponed until execution time 

the loader locates the dll, loading it into memory if necessary. It then adjusts addresses that reference functions in the dynamic library. 

### Advantage of DLL
- small executable image
- efficient usage of main memory 
- efficient sharing of libraries among multiple processes
	- only one instance of the dll in main memory when they are shared 

### Stub 
a piece of code created from compiling when there are functions or data from the dll or a place holder for the actual functions or data
- with executing the stub, the process loads the corresponding functions or data from the dll if its not present or refers to the corresponding memory block if already updated 
- the stub is then **replaced** with the corresponding function or data i.e. the stub is replaced with a direct reference to the memory address of the function or data within the dll 
### Contiguous Memory Allocation 
the old way of doing it. Two partitions are created, one for the OS and the other for user processes. it is inefficient since sometimes there is not enough contiguous space available for some processes
![[Screen Shot 2024-03-13 at 10.53.08 AM.png]]

### Fragmentation 
- External Fragmentation - in the case that there is enough total memory space to satisfy a request but the available spaces are not contiguous
- with time, the free memory spaces is broken into little pieces 
- fixed size memory partition allocation - breaks the physical memory into fixed-size blocks and allocates memory units based on block size 


### Solution to Fragmentation 
we can compact all non-contiguous memory used by a process into one contiguous process. We shuffle the memory  contents so as to place all free memory together in one large block
- applicable to dynamic relocation
- this scheme can be expensive in terms of overhead 

### Paging 
permits a process's physical address space to be noncontiguous 
- a method to avoid external fragmentation 
-   A method to avoid external fragmentation and the associated need for  
compaction

**Frame** - a base atomic unit of physical memory (some size thats a power of 2)
**Page** - an atomic of unit of logical memory allocation 

for paging to work we set up a page table to translate logical address to physical addresses. We still have internal fragmentation and each process has its own page tables to keep track of its memory allocation without interfering with other processes. 

### Paging Address Translation
address generated by the cpu is divided into 2 spaces 
- page number (p)
- page offset (d) 

### Address Translation Scheme 
addresses generated by the CPU is divided into:
- page number: an index into a page table which contains base addresses of each frame in physical memory
- page offset: the location in the frame being referenced 

These tasks are handled by the MMU of the CPU. 
1. mmu reads logical address and converts it into page number
2. calculate base address on the smallest address in the page table base register, page number and page size
3. add the offset to get the location within the frame 
![[Screen Shot 2024-03-15 at 10.27.07 AM.png]]

### Page Table Creation Procedure
1. a processes arrives with a memory requirement of n-pages 
2. if physical frames for n pages are available, they are allocated to this process
	1. the mapping between the page and assigned frame is updated at the page table as the first page of the process is loaded into one of the allocated frames
	2. the next page is loaded into another frame, its frame number is put into the page table and so on

### Hardware Support for Paging 
translation look-aside buffer (TLB) 
- a buffer that stores recently accessed virtual-to-physical address translations 
- a small, high-speed cache in the memory management unit 
	- reduces time for address translation 
	- stores a subset of the entries from the page table (typically, 100 entries or so)


![[Screen Shot 2024-03-15 at 10.33.12 AM.png]]

### Structure of the Page Table
a page tabble with 32-bit logical address space and a page size of 4kb (2^12) 
- the table would have 1 million entries (2^32/2^12) 

the page table would itself take a lot of space in memory, so some solutions were devised
- hierarchical paging
- hashed page tables
- inverted page tables 

### Heirarchical Page Tables
- break up the logical address space into multiple page tables 
- a simple technique is a two-level page table
- we then page the page table 

![[Screen Shot 2024-03-15 at 10.38.29 AM.png]]

how does this save memory? We rely on the fact that processes do not use all of their allocated memory space. We don't have to keep every second-level memory table, we just need to keep the ones that are needed by processes at the time. 

### Hashed Page Table 
address translation using hash table
- virtual addresses are hashed to generate an index in the hash table 
- each entry in the hash table contains information about the mapping of virtual pages to physical frames 

### Inverted Page Table
a page table which has an entry for each page in memory. The number of frames is equal to the number of blocks of memory that we have in our system. The OS maintains a single large page table of these frames. 

- inverted because it inverts the mapping, associating physical frames with the corresponding virtual pages 
- a single table is common to all processes with
	- virtual page number (VPN)
	- process identifier (PID)
	- page frame number (PFN) 
- often implemented as a hash table based on the virtual page number and process identifier 
	- particularly useful in systems with large address spaces and relatively few active pages 

### Swapping (Process Swapping)
moving processes from a backing store into memory and vice versa
- backing store: a fast secondary storage large enough to acomodate copies of all memory images for all users
- enables the management of processes beyond the real physical memory of the system 
- increasing the degree of multiprogramming in the system
- major consideration is transfer time 

we may also swap with pages in a processes called **swapping with paging** 
- only a small number of pages will be involved in swapping. we only move certain portions of processes around so we don't have to swap the entire processes resulting in lengthy offloading and retrieval times 

**page out** - moves a page from memory to the backing store
**page in** - moves a page from the backing store to the memory 

### Segmentation 
a memory management scheme that supports user view of memory. The program is segmented into segments. A segment represents a logical unit such as:
- main program
- procedure
- method
- object
- variables
- etc. 
### Segmentation Architecture
- segment table base register (STBR) - points to a segmentation tables place in memory
- Segment-table length register (STLR) indicates number of segments  
used by a program  
-  Segment number s is legal if s < STLR  
- STBR and STLR are special registers in MMU  
58
