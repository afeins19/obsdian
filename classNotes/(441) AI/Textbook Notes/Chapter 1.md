# 1.3.4 A Dose of Reality 
	"The following statement by Herbert Simon in 1957 is often quoted: It is not my aim to surprise  or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think, that learn and that create.  Moreover, their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied."

## Overconfidence 
	as early ai algorithms were able to things computers were previously thought to never be able to accomplish, the researchers on the forefront of this field made some bold predictions of the future achievments that will be made. We were shown that although many of their predictions were correct, the time scale was quite a bit longer than they predicted. Why?

## Weak Methods
- early ai simply linked together elementary reasoning steps to arrive at a conclusion
- the following system introduces the advancements and change in direction to combat these problems - knowledge based systems 

# 1.3.5. Knowledge Based Systems

## Complexity 
- the thought at the time (1950s - early 1960s) was that a trivial increase in computation power would be all that was needed to handle more complex problems 
- this turned out to be false, as problem set size increases led to exponential growth in the possible solutions


## Expert Systems 
- start of heuristic programming project (hpp)
- started to separate the knowledge (rules of a task) from the reasoning component of the ai system 
- MYCIN: medical diagnosis AI, performed as well as experts in a particular medical field and notably better than junio doctors (this achievment required about 450 rules). 
- MYCIN differed from DENDRAL (mass spectroscopy ai) in that rules for aquiring knowledge were not based on theory...just information gathered from textbooks and experts in the medical field 
- MYCIN also. incorporated a calculus of certaint factors, which modeled the uncertainty of some the uncertainty about a given diagnosis.  This modeled how doctors assesed the impact of evidence on the diagnosis 

## 1.3.6 AI Becomes an Industry (1980 - present)
	the first commercial expert system called R1 helped configure orders for new computer systems; by 1986 it was saving DEC (Digital Equipment Coroporation) an estimated $40 million per year! 
- There was a bubble in AI tech as europe, UK,  US, and Japan created formal divisions to persue AI and Computing Technologies
- we again saw overly ambitious goals being promised
### Back-Propogation
	although first discovered in 1969,  this algorithm proved to be extremely useful for future ai research and made resurgence 

## 1.3.8 AI Adopts the Scientific Method (1987 present)
	"in the early period of AI it seemed plausible that new forms of symbolic computation, e.g., frames and semantic networks, made much of classical theory obsolete. This led to a form of isolationism in which AI became largely separated from the rest of computer science. This isolationism is currently being abandoned. There is a recognition that machine learning should not be. isolated from information theroy, that uncertain reasoning should not be isolated from stochastic modeling, that search should not be isolated from classical optimization and control, and that automated reasoning should not be isolated from formal methods and static analyis". - David McAllester 
- AI Matures to a high level of Rigor with well established mathematical proofs and concepts and empirical results published 
- sharing of repositories of data and code now became popular...making experiments easy to replicate by other researchers 

## Hidden Markov Models (HMMs)
- a solid and deliberate approach to speech recognition algorithms
- mathematically backed 
- future speech models were built on a large foundation of previous math 
- lots of data (human speech)...easy to aquire 

## The Bayesian Network 
-  future advancements began to use  well established formal mathematical models to describe AI, bayesian networks linked statistics and probability to the use of AI for reasoning in uncertain environments 
- This allows AI systems to learn from experience

# 1.3.10 The availability of very large data sets(2001 - present) 
- computer science started with a focus on algorithms
- recent work in AI suggests that for many problems, it makes more sense to worry about the data and be less picky about what algorithm to apply 
- this approach is reasonable given the size of available data sets today 
- a mediocre algorithm with 100 million words performs better than the best one with 1 million words (For language understanding)
- the direction for optimizing ai networks seems to be with optimizing learning algorithms rather than hard coded processing algorithms...let the machines figure it out lol


